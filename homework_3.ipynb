{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMuJRaXEmliOe8zsdsrWXWJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuliaZanevych/HW_for_DataTalks.Club/blob/main/homework_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's walk through these data preparation steps:\n",
        "\n",
        "    Selecting only the required features.\n",
        "    Transforming column names.\n",
        "    Filling in missing values.\n",
        "    Renaming the MSRP variable to price."
      ],
      "metadata": {
        "id": "EI9cRz6bDIPj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1G5zOXjCrIe",
        "outputId": "77a53083-a9bc-4724-cca8-ce2ae2624667"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  make       model  year  engine_hp  engine_cylinders transmission_type  \\\n",
            "0  BMW  1 Series M  2011      335.0               6.0            MANUAL   \n",
            "1  BMW    1 Series  2011      300.0               6.0            MANUAL   \n",
            "2  BMW    1 Series  2011      300.0               6.0            MANUAL   \n",
            "3  BMW    1 Series  2011      230.0               6.0            MANUAL   \n",
            "4  BMW    1 Series  2011      230.0               6.0            MANUAL   \n",
            "\n",
            "  vehicle_style  highway_mpg  city_mpg  price  \n",
            "0         Coupe           26        19  46135  \n",
            "1   Convertible           28        19  40650  \n",
            "2         Coupe           28        20  36350  \n",
            "3         Coupe           28        18  29450  \n",
            "4   Convertible           28        18  34500  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming you've already loaded the dataset into a variable called 'data'\n",
        "# If not, first load it:\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-02-car-price/data.csv\")\n",
        "\n",
        "# 1. Selecting only the required features\n",
        "selected_columns = [\n",
        "    \"Make\", \"Model\", \"Year\", \"Engine HP\", \"Engine Cylinders\",\n",
        "    \"Transmission Type\", \"Vehicle Style\", \"highway MPG\", \"city mpg\", \"MSRP\"\n",
        "]\n",
        "data = data[selected_columns]\n",
        "\n",
        "# 2. Transforming column names\n",
        "data.columns = data.columns.str.replace(' ', '_').str.lower()\n",
        "\n",
        "# 3. Filling in missing values\n",
        "data = data.fillna(0)\n",
        "\n",
        "# 4. Renaming the 'msrp' variable to 'price'\n",
        "data = data.rename(columns={\"msrp\": \"price\"})\n",
        "\n",
        "# Check the transformed data\n",
        "print(data.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1\n",
        "\n",
        "What is the most frequent observation (mode) for the column transmission_type?\n",
        "\n",
        "    AUTOMATIC\n",
        "    MANUAL\n",
        "    AUTOMATED_MANUAL\n",
        "    DIRECT_DRIVE\n"
      ],
      "metadata": {
        "id": "4dmG6GoMDFkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting the occurrences of each value in the 'transmission_type' column\n",
        "transmission_counts = data['transmission_type'].value_counts()\n",
        "\n",
        "# Getting the most frequent observation\n",
        "most_frequent_transmission = transmission_counts.idxmax()\n",
        "\n",
        "print(most_frequent_transmission)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQJdSbrgDRwk",
        "outputId": "e94f46f9-d372-4eb2-8586-b770428f0f64"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUTOMATIC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2\n",
        "\n",
        "Create the correlation matrix for the numerical features of your dataset. In a correlation matrix, you compute the correlation coefficient between every pair of features in the dataset.\n",
        "\n",
        "What are the two features that have the biggest correlation in this dataset?\n",
        "\n",
        "    engine_hp and year\n",
        "    engine_hp and engine_cylinders\n",
        "    highway_mpg and engine_cylinders\n",
        "    highway_mpg and city_mpg\n"
      ],
      "metadata": {
        "id": "6X0o2FWSDbNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the correlation matrix for the numerical features\n",
        "correlation_matrix = data.corr()\n",
        "\n",
        "# Since we're only interested in the highest absolute correlation that isn't 1,\n",
        "# we can replace the diagonal of the matrix with zeros.\n",
        "for i in range(correlation_matrix.shape[0]):\n",
        "    correlation_matrix.iloc[i, i] = 0\n",
        "\n",
        "# Finding the two features with the highest correlation\n",
        "max_corr_value = correlation_matrix.abs().max().max()\n",
        "row, col = (correlation_matrix.abs() == max_corr_value).stack().idxmax()\n",
        "\n",
        "print(f\"The two features with the highest correlation are: {row} and {col} with a correlation of {max_corr_value:.2f}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fXE4PWWDceK",
        "outputId": "193926dd-3b05-450c-9027-4694e855d04e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The two features with the highest correlation are: highway_mpg and city_mpg with a correlation of 0.89.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-57bb98af6e60>:2: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  correlation_matrix = data.corr()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make price binary\n",
        "\n",
        "    Now we need to turn the price variable from numeric into a binary format.\n",
        "    Let's create a variable above_average which is 1 if the price is above its mean value and 0 otherwise.\n"
      ],
      "metadata": {
        "id": "tozq38YrDnKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the mean of the 'price' column\n",
        "mean_price = data['price'].mean()\n",
        "\n",
        "# Create the 'above_average' column. Using numpy's where method can make this operation faster and more concise.\n",
        "import numpy as np\n",
        "data['above_average'] = np.where(data['price'] > mean_price, 1, 0)\n",
        "\n",
        "# Check the transformed data\n",
        "print(data[['price', 'above_average']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2J3qnQqDock",
        "outputId": "909061c0-4209-4232-b46f-5473c361e1bf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   price  above_average\n",
            "0  46135              1\n",
            "1  40650              1\n",
            "2  36350              0\n",
            "3  29450              0\n",
            "4  34500              0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split the data\n",
        "\n",
        "    Split your data in train/val/test sets with 60%/20%/20% distribution.\n",
        "    Use Scikit-Learn for that (the train_test_split function) and set the seed to 42.\n",
        "    Make sure that the target value (above_average) is not in your dataframe.\n"
      ],
      "metadata": {
        "id": "6eT2qQXwDuAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Separate the target variable from the data\n",
        "X = data.drop(columns=['above_average'])\n",
        "y = data['above_average']\n",
        "\n",
        "# First, split the data into an 80% train and 20% test set\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Then, split the train set again to create the validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
        "\n",
        "print(f\"Train set size: {len(X_train)}\")\n",
        "print(f\"Validation set size: {len(X_val)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vmyy63rrD4NL",
        "outputId": "b96d52df-1c3e-4447-dd41-66e1727cc2b5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set size: 7148\n",
            "Validation set size: 2383\n",
            "Test set size: 2383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3\n",
        "\n",
        "    Calculate the mutual information score between above_average and other categorical variables in our dataset. Use the training set only.\n",
        "    Round the scores to 2 decimals using round(score, 2).\n",
        "\n",
        "Which of these variables has the lowest mutual information score?\n",
        "\n",
        "    make\n",
        "    model\n",
        "    transmission_type\n",
        "    vehicle_style\n"
      ],
      "metadata": {
        "id": "a93hCX8cD5yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "# List of categorical variables\n",
        "categorical_variables = ['make', 'model', 'transmission_type', 'vehicle_style']\n",
        "\n",
        "# Calculate mutual information scores\n",
        "mi_scores = {}\n",
        "\n",
        "for var in categorical_variables:\n",
        "    le = LabelEncoder()\n",
        "    encoded = le.fit_transform(X_train[var])\n",
        "    mi = mutual_info_classif(encoded.reshape(-1, 1), y_train)\n",
        "    mi_scores[var] = round(mi[0], 2)\n",
        "\n",
        "# Finding variable with the lowest mutual information score\n",
        "lowest_mi_var = min(mi_scores, key=mi_scores.get)\n",
        "\n",
        "print(\"Mutual Information Scores:\", mi_scores)\n",
        "print(f\"The variable with the lowest mutual information score is: {lowest_mi_var}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPNbo50AD9m7",
        "outputId": "c42099ce-6c3e-48e0-b753-e06fe6e5b0c7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mutual Information Scores: {'make': 0.0, 'model': 0, 'transmission_type': 0.03, 'vehicle_style': 0.01}\n",
            "The variable with the lowest mutual information score is: make\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4\n",
        "\n",
        "    Now let's train a logistic regression.\n",
        "    Remember that we have several categorical variables in the dataset. Include them using one-hot encoding.\n",
        "    Fit the model on the training dataset.\n",
        "        To make sure the results are reproducible across different versions of Scikit-Learn, fit the model with these parameters:\n",
        "        model = LogisticRegression(solver='liblinear', C=10, max_iter=1000, random_state=42)\n",
        "    Calculate the accuracy on the validation dataset and round it to 2 decimal digits.\n",
        "\n",
        "What accuracy did you get?\n",
        "\n",
        "    0.60\n",
        "    0.72\n",
        "    0.84\n",
        "    0.95\n"
      ],
      "metadata": {
        "id": "0j9Sv6G_EVGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Combine the training and validation data for one-hot encoding\n",
        "X_combined = pd.concat([X_train, X_val])\n",
        "\n",
        "# One-hot encode the combined data\n",
        "encoder = OneHotEncoder(drop='first', sparse=False)\n",
        "X_combined_encoded = encoder.fit_transform(X_combined[categorical_variables])\n",
        "\n",
        "# Split the combined one-hot encoded data back into training and validation sets\n",
        "X_train_encoded = X_combined_encoded[:len(X_train)]\n",
        "X_val_encoded = X_combined_encoded[len(X_train):]\n",
        "\n",
        "X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoder.get_feature_names_out(categorical_variables), index=X_train.index)\n",
        "X_val_encoded_df = pd.DataFrame(X_val_encoded, columns=encoder.get_feature_names_out(categorical_variables), index=X_val.index)\n",
        "\n",
        "# Replace categorical variables with their one-hot encoded representations in the original dataframes\n",
        "X_train_ohe = pd.concat([X_train.drop(columns=categorical_variables), X_train_encoded_df], axis=1)\n",
        "X_val_ohe = pd.concat([X_val.drop(columns=categorical_variables), X_val_encoded_df], axis=1)\n",
        "\n",
        "# Train the logistic regression model\n",
        "model = LogisticRegression(solver='liblinear', C=10, max_iter=1000, random_state=42)\n",
        "model.fit(X_train_ohe, y_train)\n",
        "\n",
        "# Calculate the accuracy on the validation dataset\n",
        "y_pred_val = model.predict(X_val_ohe)\n",
        "accuracy = accuracy_score(y_val, y_pred_val)\n",
        "rounded_accuracy = round(accuracy, 2)\n",
        "\n",
        "print(f\"Accuracy on the validation dataset: {rounded_accuracy}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vu3dGJ9WEWL8",
        "outputId": "d8c8103e-3740-498e-9e4d-093f1672a0f2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the validation dataset: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5\n",
        "\n",
        "    Let's find the least useful feature using the feature elimination technique.\n",
        "    Train a model with all these features (using the same parameters as in Q4).\n",
        "    Now exclude each feature from this set and train a model without it. Record the accuracy for each model.\n",
        "    For each feature, calculate the difference between the original accuracy and the accuracy without the feature.\n",
        "\n",
        "Which of following feature has the smallest difference?\n",
        "\n",
        "    year\n",
        "    engine_hp\n",
        "    transmission_type\n",
        "    city_mpg\n",
        "\n",
        "    Note: the difference doesn't have to be positive\n"
      ],
      "metadata": {
        "id": "Z3sl1MT6Ew3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model using all features\n",
        "model_all = LogisticRegression(solver='liblinear', C=10, max_iter=1000, random_state=42)\n",
        "model_all.fit(X_train_ohe, y_train)\n",
        "y_pred_val_all = model_all.predict(X_val_ohe)\n",
        "accuracy_all = accuracy_score(y_val, y_pred_val_all)\n",
        "\n",
        "# List of features for evaluation\n",
        "features_to_evaluate = ['year', 'engine_hp', 'transmission_type', 'city_mpg']\n",
        "differences = {}\n",
        "\n",
        "# Iterate over each feature, exclude it, train the model, and compute accuracy difference\n",
        "for feature in features_to_evaluate:\n",
        "\n",
        "    # Prepare the data by excluding the current feature\n",
        "    if feature in categorical_variables: # if it's categorical, we need to drop the encoded columns\n",
        "        columns_to_drop = [col for col in X_train_ohe.columns if feature in col]\n",
        "        X_train_without_feature = X_train_ohe.drop(columns=columns_to_drop)\n",
        "        X_val_without_feature = X_val_ohe.drop(columns=columns_to_drop)\n",
        "    else:\n",
        "        X_train_without_feature = X_train_ohe.drop(columns=feature)\n",
        "        X_val_without_feature = X_val_ohe.drop(columns=feature)\n",
        "\n",
        "    # Train the model\n",
        "    model_without_feature = LogisticRegression(solver='liblinear', C=10, max_iter=1000, random_state=42)\n",
        "    model_without_feature.fit(X_train_without_feature, y_train)\n",
        "\n",
        "    # Predict and compute accuracy\n",
        "    y_pred_val_without_feature = model_without_feature.predict(X_val_without_feature)\n",
        "    accuracy_without_feature = accuracy_score(y_val, y_pred_val_without_feature)\n",
        "\n",
        "    # Compute the difference and store\n",
        "    differences[feature] = accuracy_all - accuracy_without_feature\n",
        "\n",
        "# Identify the feature with the smallest difference\n",
        "smallest_difference_feature = min(differences, key=differences.get)\n",
        "\n",
        "print(\"Differences in Accuracy:\", differences)\n",
        "print(f\"The feature with the smallest difference in accuracy when excluded is: {smallest_difference_feature}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-M2X8KGEylz",
        "outputId": "1fe79d4d-51ac-4ecc-f24a-cdfb91dea8a0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Differences in Accuracy: {'year': 0.052035249685270624, 'engine_hp': -0.00041963911036513313, 'transmission_type': 0.0, 'city_mpg': 0.0}\n",
            "The feature with the smallest difference in accuracy when excluded is: engine_hp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6\n",
        "\n",
        "    For this question, we'll see how to use a linear regression model from Scikit-Learn.\n",
        "    We'll need to use the original column price. Apply the logarithmic transformation to this column.\n",
        "    Fit the Ridge regression model on the training data with a solver 'sag'. Set the seed to 42.\n",
        "    This model also has a parameter alpha. Let's try the following values: [0, 0.01, 0.1, 1, 10].\n",
        "    Round your RMSE scores to 3 decimal digits.\n",
        "\n",
        "Which of these alphas leads to the best RMSE on the validation set?\n",
        "\n",
        "    0\n",
        "    0.01\n",
        "    0.1\n",
        "    1\n",
        "    10\n",
        "\n",
        "    Note: If there are multiple options, select the smallest alpha.\n"
      ],
      "metadata": {
        "id": "ODH_CbLyE_pA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Extracting the 'price' column and applying the logarithmic transformation\n",
        "y_train_log = np.log1p(X_train['price'])\n",
        "y_val_log = np.log1p(X_val['price'])\n",
        "\n",
        "# Removing the original 'price' column from datasets\n",
        "X_train = X_train.drop(columns=['price'])\n",
        "X_val = X_val.drop(columns=['price'])\n",
        "\n",
        "# List of alphas for evaluation\n",
        "alphas = [0, 0.01, 0.1, 1, 10]\n",
        "rmse_scores = {}\n",
        "\n",
        "# Train Ridge regression models for each alpha value\n",
        "for alpha in alphas:\n",
        "\n",
        "    # Train the Ridge model\n",
        "    ridge = Ridge(alpha=alpha, solver='sag', random_state=42)\n",
        "    ridge.fit(X_train_ohe, y_train_log)  # Use one-hot encoded data for training\n",
        "\n",
        "    # Predict on the validation set and compute RMSE\n",
        "    y_pred_val = ridge.predict(X_val_ohe)  # Use one-hot encoded data for prediction\n",
        "    rmse = mean_squared_error(y_val_log, y_pred_val, squared=False)\n",
        "    rmse_scores[alpha] = round(rmse, 3)\n",
        "\n",
        "# Identify the alpha value with the lowest RMSE\n",
        "best_alpha = min(rmse_scores, key=rmse_scores.get)\n",
        "\n",
        "print(\"RMSE Scores for each alpha:\", rmse_scores)\n",
        "print(f\"The alpha value that leads to the lowest RMSE on the validation set is: {best_alpha}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhdEEjpYFHxJ",
        "outputId": "ae78a332-a9bf-413c-8114-dd52dbb6af94"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE Scores for each alpha: {0: 0.952, 0.01: 0.952, 0.1: 0.952, 1: 0.952, 10: 0.952}\n",
            "The alpha value that leads to the lowest RMSE on the validation set is: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}